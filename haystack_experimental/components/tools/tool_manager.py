import json
from collections import defaultdict
from dataclasses import asdict
from typing import Any, Dict, List, Optional, Union, Tuple

from haystack import component, default_from_dict, default_to_dict, logging
from haystack.components.generators.chat import OpenAIChatGenerator
from haystack.core.serialization import import_class_by_name
from haystack.dataclasses import ChatMessage
from haystack.lazy_imports import LazyImport
from haystack.utils import deserialize_secrets_inplace

from haystack_experimental.components.tools import Tool
from haystack_experimental.components.tools.openapi import LLMProvider
from haystack_experimental.components.tools.openapi._payload_extraction import create_function_payload_extractor
from haystack_experimental.components.tools.types import OpenAPIToolConfig, FunctionToolConfig, ToolConfig
from haystack_experimental.util import serialize_secrets_inplace

with LazyImport("Run 'pip install anthropic-haystack'") as anthropic_import:
    # pylint: disable=import-error
    from haystack_integrations.components.generators.anthropic import AnthropicChatGenerator

with LazyImport("Run 'pip install cohere-haystack'") as cohere_import:
    # pylint: disable=import-error
    from haystack_integrations.components.generators.cohere import CohereChatGenerator

logger = logging.getLogger(__name__)


def find_function_name_path(tool_dict: Dict[str, Any]) -> Optional[List[str]]:
    def search(d: Dict[str, Any], path: List[str]) -> Optional[List[str]]:
        if isinstance(d, dict):
            if "name" in d and "description" in d:
                return path + ["name"]
            for k, v in d.items():
                result = search(v, path + [k])
                if result:
                    return result
        return None

    return search(tool_dict, [])


def get_nested(d: Dict[str, Any], path: List[str]) -> Any:
    for key in path:
        d = d[key]
    return d


@component
class ToolManager:
    def __init__(
        self,
        tools: List[ToolConfig],
        generator_api: LLMProvider = LLMProvider.OPENAI,
        generator_api_params: Optional[Dict[str, Any]] = None,
    ):
        self.function_map: Dict[str, Tool] = {}
        self.tools_definitions: List[Dict[str, Any]] = []

        for tool_config in tools:
            tool_handler_class_name: str = tool_config.handler
            if not tool_handler_class_name:
                logger.warning(f"Tool handler not specified in config: {tool_config}. Skipping this tool.")
                continue

            try:
                tool_handler_class = import_class_by_name(tool_handler_class_name)
                if not issubclass(tool_handler_class, Tool):
                    logger.warning(f"{tool_handler_class_name} is not a subclass of Tool. Skipping this tool.")
                    continue
                tool_config = {**tool_config.get_config(), **{"llm_provider":generator_api}}
                tool: Tool = tool_handler_class(**tool_config)
                tool_definitions = tool.get_tools_definitions()
                for tool_def in tool_definitions:
                    path_to_tool_name = find_function_name_path(tool_def)
                    if path_to_tool_name:
                        self.function_map[get_nested(tool_def, path_to_tool_name)] = tool
                self.tools_definitions.extend(tool_definitions)
                logger.info(f"Successfully initialized tool: {tool_handler_class_name}")
            except Exception as e:
                logger.error(f"Failed to initialize tool {tool_handler_class_name}. Error: {str(e)}")

        self.tools = tools
        self.generator_api = generator_api
        self.generator_api_params = generator_api_params or {}
        self.chat_generator = self._init_generator(generator_api, self.generator_api_params)

    def _init_generator(self, generator_api: LLMProvider, generator_api_params: Dict[str, Any]):
        if generator_api == LLMProvider.OPENAI:
            return OpenAIChatGenerator(**generator_api_params)
        if generator_api == LLMProvider.COHERE:
            cohere_import.check()
            return CohereChatGenerator(**generator_api_params)
        if generator_api == LLMProvider.ANTHROPIC:
            anthropic_import.check()
            return AnthropicChatGenerator(**generator_api_params)
        raise ValueError(f"Unsupported generator API: {generator_api}")

    @component.output_types(service_response=List[ChatMessage])
    def run(
        self, messages: List[ChatMessage], fc_generator_kwargs: Optional[Dict[str, Any]] = None
    ) -> Dict[str, List[ChatMessage]]:
        """
        Invokes the underlying OpenAPI service/tool with the function calling payload generated by the chat generator.

        :param messages: List of ChatMessages to generate function calling payload (e.g. human instructions). The last
        message should be human instruction containing enough information to generate the function calling payload
        suitable for the OpenAPI service/tool used. See the examples in the class docstring.
        :param fc_generator_kwargs: Additional arguments for the function calling payload generation process.
        :returns: The response from the service after invoking the function.
        """
        provider_to_arguments_field_name = defaultdict(
            lambda: "arguments",
            {
                LLMProvider.ANTHROPIC: "input",
                LLMProvider.COHERE: "parameters",
            },
        )

        fc_generator_kwargs = fc_generator_kwargs or {}
        fc_generator_kwargs["tools"] = self.tools_definitions

        fc_payload = self.chat_generator.run(messages, fc_generator_kwargs)
        try:
            invocation_payload = json.loads(fc_payload["replies"][0].content)
            resolve_payload_fn = create_function_payload_extractor(provider_to_arguments_field_name[self.generator_api])
            invocation_payload_resolved = resolve_payload_fn(invocation_payload)
            if "name" not in invocation_payload_resolved or "arguments" not in invocation_payload_resolved:
                raise ValueError(
                    f"Function invocation payload does not contain 'name' or 'arguments' keys: {invocation_payload}, "
                    f"the payload extraction function may be incorrect."
                )

            if invocation_payload_resolved["name"] not in self.function_map:
                raise ValueError(f"Unknown function mapping")

            invoker = self.function_map[invocation_payload_resolved["name"]]
            service_response = invoker.invoke(invocation_payload)
        except Exception as e:
            service_response = {"error": str(e)}

        response_messages = [ChatMessage.from_user(json.dumps(service_response))]
        return {"service_response": response_messages}

    def to_dict(self) -> Dict[str, Any]:
        """
        Serialize this component to a dictionary.

        :returns:
            The serialized component as a dictionary.
        """
        serialize_secrets_inplace(self.generator_api_params, keys=["api_key"], recursive=True)
        return default_to_dict(
            self,
            tools=[asdict(t) for t in self.tools],
            generator_api=self.generator_api.value,
            generator_api_params=self.generator_api_params,
        )

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ToolManager":
        """
        Deserialize this component from a dictionary.

        :param data: The dictionary representation of this component.
        :returns:
            The deserialized component instance.
        """
        init_params = data.get("init_parameters", {})
        generator_api = init_params.get("generator_api")
        data["init_parameters"]["generator_api"] = LLMProvider.from_str(generator_api)
        return default_from_dict(cls, data)
