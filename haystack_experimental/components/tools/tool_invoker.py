# SPDX-FileCopyrightText: 2022-present deepset GmbH <info@deepset.ai>
#
# SPDX-License-Identifier: Apache-2.0

import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Optional, Union

from haystack.components.agents import State
from haystack.components.tools.tool_invoker import ToolInvoker as HaystackToolInvoker
from haystack.components.tools.tool_invoker import ToolOutputMergeError
from haystack.core.component.component import component
from haystack.core.errors import DeserializationError
from haystack.core.serialization import default_from_dict, default_to_dict, import_class_by_name, logging
from haystack.dataclasses import ChatMessage, ToolCall
from haystack.dataclasses.streaming_chunk import StreamingCallbackT, StreamingChunk, select_streaming_callback
from haystack.tools import (
    Tool,
    Toolset,
    deserialize_tools_or_toolset_inplace,
    serialize_tools_or_toolset,
)
from haystack.tools.errors import ToolInvocationError
from haystack.utils.callable_serialization import deserialize_callable, serialize_callable

from haystack_experimental.components.agents.human_in_the_loop.types import ConfirmationStrategy

logger = logging.getLogger(__name__)


@component
class ToolInvoker(HaystackToolInvoker):
    """
    Invokes tools based on prepared tool calls and returns the results as a list of ChatMessage objects.

    Also handles reading/writing from a shared `State`.
    At initialization, the ToolInvoker component is provided with a list of available tools.
    At runtime, the component processes a list of ChatMessage object containing tool calls
    and invokes the corresponding tools.
    The results of the tool invocations are returned as a list of ChatMessage objects with tool role.

    Usage example:
    ```python
    from haystack.dataclasses import ChatMessage, ToolCall
    from haystack.tools import Tool
    from haystack.components.tools import ToolInvoker

    # Tool definition
    def dummy_weather_function(city: str):
        return f"The weather in {city} is 20 degrees."

    parameters = {"type": "object",
                "properties": {"city": {"type": "string"}},
                "required": ["city"]}

    tool = Tool(name="weather_tool",
                description="A tool to get the weather",
                function=dummy_weather_function,
                parameters=parameters)

    # Usually, the ChatMessage with tool_calls is generated by a Language Model
    # Here, we create it manually for demonstration purposes
    tool_call = ToolCall(
        tool_name="weather_tool",
        arguments={"city": "Berlin"}
    )
    message = ChatMessage.from_assistant(tool_calls=[tool_call])

    # ToolInvoker initialization and run
    invoker = ToolInvoker(tools=[tool])
    result = invoker.run(messages=[message])

    print(result)
    ```

    ```
    >>  {
    >>      'tool_messages': [
    >>          ChatMessage(
    >>              _role=<ChatRole.TOOL: 'tool'>,
    >>              _content=[
    >>                  ToolCallResult(
    >>                      result='"The weather in Berlin is 20 degrees."',
    >>                      origin=ToolCall(
    >>                          tool_name='weather_tool',
    >>                          arguments={'city': 'Berlin'},
    >>                          id=None
    >>                      )
    >>                  )
    >>              ],
    >>              _meta={}
    >>          )
    >>      ]
    >>  }
    ```

    Usage example with a Toolset:
    ```python
    from haystack.dataclasses import ChatMessage, ToolCall
    from haystack.tools import Tool, Toolset
    from haystack.components.tools import ToolInvoker

    # Tool definition
    def dummy_weather_function(city: str):
        return f"The weather in {city} is 20 degrees."

    parameters = {"type": "object",
                "properties": {"city": {"type": "string"}},
                "required": ["city"]}

    tool = Tool(name="weather_tool",
                description="A tool to get the weather",
                function=dummy_weather_function,
                parameters=parameters)

    # Create a Toolset
    toolset = Toolset([tool])

    # Usually, the ChatMessage with tool_calls is generated by a Language Model
    # Here, we create it manually for demonstration purposes
    tool_call = ToolCall(
        tool_name="weather_tool",
        arguments={"city": "Berlin"}
    )
    message = ChatMessage.from_assistant(tool_calls=[tool_call])

    # ToolInvoker initialization and run with Toolset
    invoker = ToolInvoker(tools=toolset)
    result = invoker.run(messages=[message])

    print(result)
    """

    def __init__(
        self,
        tools: Union[list[Tool], Toolset],
        raise_on_failure: bool = True,
        convert_result_to_json_string: bool = False,
        streaming_callback: Optional[StreamingCallbackT] = None,
        *,
        enable_streaming_callback_passthrough: bool = False,
        max_workers: int = 4,
        confirmation_strategies: Optional[dict[str, ConfirmationStrategy]] = None,
    ):
        """
        Initialize the ToolInvoker component.

        :param tools:
            A list of tools that can be invoked or a Toolset instance that can resolve tools.
        :param raise_on_failure:
            If True, the component will raise an exception in case of errors
            (tool not found, tool invocation errors, tool result conversion errors).
            If False, the component will return a ChatMessage object with `error=True`
            and a description of the error in `result`.
        :param convert_result_to_json_string:
            If True, the tool invocation result will be converted to a string using `json.dumps`.
            If False, the tool invocation result will be converted to a string using `str`.
        :param streaming_callback:
            A callback function that will be called to emit tool results.
            Note that the result is only emitted once it becomes available — it is not
            streamed incrementally in real time.
        :param enable_streaming_callback_passthrough:
            If True, the `streaming_callback` will be passed to the tool invocation if the tool supports it.
            This allows tools to stream their results back to the client.
            Note that this requires the tool to have a `streaming_callback` parameter in its `invoke` method signature.
            If False, the `streaming_callback` will not be passed to the tool invocation.
        :param max_workers:
            The maximum number of workers to use in the thread pool executor.
            This also decides the maximum number of concurrent tool invocations.
        :param confirmation_strategies:
            A dictionary mapping tool names to their corresponding confirmation strategies.
            If a tool name is present in this dictionary, the associated confirmation strategy
            will be executed before invoking the tool.
            If the tool execution is rejected by the strategy, the tool will not be invoked,
            and an appropriate message will be returned instead.
        :raises ValueError:
            If no tools are provided or if duplicate tool names are found.
        """
        super(ToolInvoker, self).__init__(
            tools=tools,
            raise_on_failure=raise_on_failure,
            convert_result_to_json_string=convert_result_to_json_string,
            streaming_callback=streaming_callback,
            enable_streaming_callback_passthrough=enable_streaming_callback_passthrough,
            max_workers=max_workers,
        )
        self.confirmation_strategies = confirmation_strategies or {}

    def _handle_confirmation_strategies(
        self, tool_calls: list[ToolCall], tool_call_params: list[dict[str, Any]]
    ) -> tuple[list[ToolCall], list[dict[str, Any]], list[ChatMessage]]:
        confirmed_tool_calls = []
        confirmed_tool_call_params = []
        tool_messages = []
        for tool_call, params in zip(tool_calls, tool_call_params):
            tool_name = params["tool_to_invoke"].name

            # If no confirmation strategy is defined for this tool, proceed with execution
            if tool_name not in self.confirmation_strategies:
                confirmed_tool_calls.append(tool_call)
                confirmed_tool_call_params.append(params)
                continue

            tool_execution_decision = self.confirmation_strategies[tool_name].run(
                tool=params["tool_to_invoke"], tool_params=params["final_args"]
            )

            if tool_execution_decision.execute:
                # TODO Need to figure out a way to forward the feedback message here if not empty
                #      This is relevant if tool params were modified by the user
                # additional_feedback = tool_execution_decision.feedback
                params["final_args"].update(tool_execution_decision.final_tool_params)
                confirmed_tool_calls.append(tool_call)
                confirmed_tool_call_params.append(params)
            else:
                # Tool execution was rejected with a message
                tool_messages.append(
                    ChatMessage.from_tool(
                        tool_result=tool_execution_decision.feedback or "",
                        origin=tool_call,
                        error=True,
                    )
                )

        return confirmed_tool_calls, confirmed_tool_call_params, tool_messages

    @component.output_types(tool_messages=list[ChatMessage], state=State)
    def run(
        self,
        messages: list[ChatMessage],
        state: Optional[State] = None,
        streaming_callback: Optional[StreamingCallbackT] = None,
        *,
        enable_streaming_callback_passthrough: Optional[bool] = None,
        tools: Optional[Union[list[Tool], Toolset]] = None,
    ) -> dict[str, Any]:
        """
        Processes ChatMessage objects containing tool calls and invokes the corresponding tools, if available.

        :param messages:
            A list of ChatMessage objects.
        :param state: The runtime state that should be used by the tools.
        :param streaming_callback: A callback function that will be called to emit tool results.
            Note that the result is only emitted once it becomes available — it is not
            streamed incrementally in real time.
        :param enable_streaming_callback_passthrough:
            If True, the `streaming_callback` will be passed to the tool invocation if the tool supports it.
            This allows tools to stream their results back to the client.
            Note that this requires the tool to have a `streaming_callback` parameter in its `invoke` method signature.
            If False, the `streaming_callback` will not be passed to the tool invocation.
            If None, the value from the constructor will be used.
        :param tools:
            A list of tools to use for the tool invoker. If set, overrides the tools set in the constructor.
        :returns:
            A dictionary with the key `tool_messages` containing a list of ChatMessage objects with tool role.
            Each ChatMessage objects wraps the result of a tool invocation.

        :raises ToolNotFoundException:
            If the tool is not found in the list of available tools and `raise_on_failure` is True.
        :raises ToolInvocationError:
            If the tool invocation fails and `raise_on_failure` is True.
        :raises StringConversionError:
            If the conversion of the tool result to a string fails and `raise_on_failure` is True.
        :raises ToolOutputMergeError:
            If merging tool outputs into state fails and `raise_on_failure` is True.
        """
        tools_with_names = self._tools_with_names
        if tools is not None:
            tools_with_names = self._validate_and_prepare_tools(tools)
            logger.debug(
                f"For this invocation, overriding constructor tools with: {', '.join(tools_with_names.keys())}"
            )

        if state is None:
            state = State(schema={})

        resolved_enable_streaming_passthrough = (
            enable_streaming_callback_passthrough
            if enable_streaming_callback_passthrough is not None
            else self.enable_streaming_callback_passthrough
        )

        # Only keep messages with tool calls
        messages_with_tool_calls = [message for message in messages if message.tool_calls]
        streaming_callback = select_streaming_callback(
            init_callback=self.streaming_callback, runtime_callback=streaming_callback, requires_async=False
        )

        if not messages_with_tool_calls:
            return {"tool_messages": [], "state": state}

        # 1) Collect all tool calls and their parameters for parallel execution
        tool_messages = []
        tool_calls, tool_call_params, error_messages = self._prepare_tool_call_params(
            messages_with_tool_calls=messages_with_tool_calls,
            state=state,
            streaming_callback=streaming_callback,
            enable_streaming_passthrough=resolved_enable_streaming_passthrough,
            tools_with_names=tools_with_names,
        )
        tool_messages.extend(error_messages)

        # Early exit if no valid tool calls
        if not tool_call_params:
            return {"tool_messages": tool_messages, "state": state}

        # Apply confirmation strategies before executing tool calls
        tool_calls, tool_call_params, rejection_messages = self._handle_confirmation_strategies(
            tool_calls=tool_calls, tool_call_params=tool_call_params
        )
        tool_messages.extend(rejection_messages)

        # Early exit if no valid tool calls after confirmation strategies
        if not tool_call_params:
            return {"tool_messages": tool_messages, "state": state}

        # 2) Execute valid tool calls in parallel
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            for params in tool_call_params:
                callable_ = self._make_context_bound_invoke(params["tool_to_invoke"], params["final_args"])
                futures.append(executor.submit(callable_))

            # 3) Gather and process results: handle errors and merge outputs into state
            for future, tool_call in zip(futures, tool_calls):
                result = future.result()

                if isinstance(result, ToolInvocationError):
                    # a) This is an error, create error Tool message
                    if self.raise_on_failure:
                        raise result
                    logger.error("{error_exception}", error_exception=result)
                    tool_messages.append(ChatMessage.from_tool(tool_result=str(result), origin=tool_call, error=True))
                else:
                    # b) In case of success, merge outputs into state
                    try:
                        tool_to_invoke = tools_with_names[tool_call.tool_name]
                        self._merge_tool_outputs(tool=tool_to_invoke, result=result, state=state)
                        tool_messages.append(
                            self._prepare_tool_result_message(
                                result=result, tool_call=tool_call, tool_to_invoke=tool_to_invoke
                            )
                        )
                    except Exception as e:
                        error = ToolOutputMergeError(
                            f"Failed to merge tool outputs from tool {tool_call.tool_name} into State: {e}"
                        )
                        if self.raise_on_failure:
                            raise error from e
                        logger.error("{error_exception}", error_exception=error)
                        tool_messages.append(
                            ChatMessage.from_tool(tool_result=str(error), origin=tool_call, error=True)
                        )

                # c) Handle streaming callback
                if streaming_callback is not None:
                    streaming_callback(
                        self._create_tool_result_streaming_chunk(tool_messages=tool_messages, tool_call=tool_call)
                    )

        # We stream one more chunk that contains a finish_reason if tool_messages were generated
        if len(tool_messages) > 0 and streaming_callback is not None:
            streaming_callback(
                StreamingChunk(
                    content="", finish_reason="tool_call_results", meta={"finish_reason": "tool_call_results"}
                )
            )

        return {"tool_messages": tool_messages, "state": state}

    @component.output_types(tool_messages=list[ChatMessage], state=State)
    async def run_async(
        self,
        messages: list[ChatMessage],
        state: Optional[State] = None,
        streaming_callback: Optional[StreamingCallbackT] = None,
        *,
        enable_streaming_callback_passthrough: Optional[bool] = None,
        tools: Optional[Union[list[Tool], Toolset]] = None,
    ) -> dict[str, Any]:
        """
        Asynchronously processes ChatMessage objects containing tool calls.

        Multiple tool calls are performed concurrently.
        :param messages:
            A list of ChatMessage objects.
        :param state: The runtime state that should be used by the tools.
        :param streaming_callback: An asynchronous callback function that will be called to emit tool results.
            Note that the result is only emitted once it becomes available — it is not
            streamed incrementally in real time.
        :param enable_streaming_callback_passthrough:
            If True, the `streaming_callback` will be passed to the tool invocation if the tool supports it.
            This allows tools to stream their results back to the client.
            Note that this requires the tool to have a `streaming_callback` parameter in its `invoke` method signature.
            If False, the `streaming_callback` will not be passed to the tool invocation.
            If None, the value from the constructor will be used.
        :param tools:
            A list of tools to use for the tool invoker. If set, overrides the tools set in the constructor.
        :returns:
            A dictionary with the key `tool_messages` containing a list of ChatMessage objects with tool role.
            Each ChatMessage objects wraps the result of a tool invocation.

        :raises ToolNotFoundException:
            If the tool is not found in the list of available tools and `raise_on_failure` is True.
        :raises ToolInvocationError:
            If the tool invocation fails and `raise_on_failure` is True.
        :raises StringConversionError:
            If the conversion of the tool result to a string fails and `raise_on_failure` is True.
        :raises ToolOutputMergeError:
            If merging tool outputs into state fails and `raise_on_failure` is True.
        """

        tools_with_names = self._tools_with_names
        if tools is not None:
            tools_with_names = self._validate_and_prepare_tools(tools)
            logger.debug(
                f"For this invocation, overriding constructor tools with: {', '.join(tools_with_names.keys())}"
            )

        if state is None:
            state = State(schema={})

        resolved_enable_streaming_passthrough = (
            enable_streaming_callback_passthrough
            if enable_streaming_callback_passthrough is not None
            else self.enable_streaming_callback_passthrough
        )

        # Only keep messages with tool calls
        messages_with_tool_calls = [message for message in messages if message.tool_calls]
        streaming_callback = select_streaming_callback(
            init_callback=self.streaming_callback, runtime_callback=streaming_callback, requires_async=True
        )

        if not messages_with_tool_calls:
            return {"tool_messages": [], "state": state}

        # 1) Collect all tool calls and their parameters for parallel execution
        tool_messages = []
        tool_calls, tool_call_params, error_messages = self._prepare_tool_call_params(
            messages_with_tool_calls=messages_with_tool_calls,
            state=state,
            streaming_callback=streaming_callback,
            enable_streaming_passthrough=resolved_enable_streaming_passthrough,
            tools_with_names=tools_with_names,
        )
        tool_messages.extend(error_messages)

        # Early exit if no valid tool calls
        if not tool_call_params:
            return {"tool_messages": tool_messages, "state": state}

        # Apply confirmation strategies before executing tool calls
        tool_calls, tool_call_params, rejection_messages = self._handle_confirmation_strategies(
            tool_calls=tool_calls, tool_call_params=tool_call_params
        )
        tool_messages.extend(rejection_messages)

        # Early exit if no valid tool calls after confirmation strategies
        if not tool_call_params:
            return {"tool_messages": tool_messages, "state": state}

        # 2) Execute valid tool calls in parallel
        tool_call_tasks = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            for params in tool_call_params:
                loop = asyncio.get_running_loop()
                callable_ = ToolInvoker._make_context_bound_invoke(params["tool_to_invoke"], params["final_args"])
                tool_call_tasks.append(loop.run_in_executor(executor, callable_))

            # 3) Gather and process results: handle errors and merge outputs into state
            tool_results = await asyncio.gather(*tool_call_tasks)
            for tool_result, tool_call in zip(tool_results, tool_calls):
                # a) This is an error, create error Tool message
                if isinstance(tool_result, ToolInvocationError):
                    if self.raise_on_failure:
                        raise tool_result
                    logger.error("{error_exception}", error_exception=tool_result)
                    tool_messages.append(
                        ChatMessage.from_tool(tool_result=str(tool_result), origin=tool_call, error=True)
                    )
                else:
                    # b) In case of success, merge outputs into state
                    try:
                        tool_to_invoke = tools_with_names[tool_call.tool_name]
                        self._merge_tool_outputs(tool=tool_to_invoke, result=tool_result, state=state)
                        tool_messages.append(
                            self._prepare_tool_result_message(
                                result=tool_result, tool_call=tool_call, tool_to_invoke=tool_to_invoke
                            )
                        )
                    except Exception as e:
                        error = ToolOutputMergeError(
                            f"Failed to merge tool outputs from tool {tool_call.tool_name} into State: {e}"
                        )
                        if self.raise_on_failure:
                            raise error from e
                        logger.error("{error_exception}", error_exception=error)
                        tool_messages.append(
                            ChatMessage.from_tool(tool_result=str(error), origin=tool_call, error=True)
                        )

                # c) Handle streaming callback
                if streaming_callback is not None:
                    await streaming_callback(
                        self._create_tool_result_streaming_chunk(tool_messages=tool_messages, tool_call=tool_call)
                    )

        # 4) We stream one more chunk that contains a finish_reason if tool_messages were generated
        if len(tool_messages) > 0 and streaming_callback is not None:
            await streaming_callback(
                StreamingChunk(
                    content="", finish_reason="tool_call_results", meta={"finish_reason": "tool_call_results"}
                )
            )

        return {"tool_messages": tool_messages, "state": state}

    def to_dict(self) -> dict[str, Any]:
        """
        Serializes the component to a dictionary.

        :returns:
            Dictionary with serialized data.
        """
        if self.streaming_callback is not None:
            streaming_callback = serialize_callable(self.streaming_callback)
        else:
            streaming_callback = None

        return default_to_dict(
            self,
            tools=serialize_tools_or_toolset(self.tools),
            raise_on_failure=self.raise_on_failure,
            convert_result_to_json_string=self.convert_result_to_json_string,
            streaming_callback=streaming_callback,
            enable_streaming_callback_passthrough=self.enable_streaming_callback_passthrough,
            max_workers=self.max_workers,
            confirmation_strategies={name: c.to_dict() for name, c in self.confirmation_strategies.items()},
        )

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "ToolInvoker":
        """
        Deserializes the component from a dictionary.

        :param data:
            The dictionary to deserialize from.
        :returns:
            The deserialized component.
        """
        deserialize_tools_or_toolset_inplace(data["init_parameters"], key="tools")
        if data["init_parameters"].get("streaming_callback") is not None:
            data["init_parameters"]["streaming_callback"] = deserialize_callable(
                data["init_parameters"]["streaming_callback"]
            )

        deserialized_strats: dict[str, ConfirmationStrategy] = {}
        if confirmation_strategies_data := data["init_parameters"].get("confirmation_strategies"):
            for tool_name, strat in confirmation_strategies_data.items():
                strat_class = import_class_by_name(strat["type"])
                if hasattr(strat_class, "from_dict"):
                    deserialized_strats[tool_name] = strat_class.from_dict(strat)
                else:
                    raise DeserializationError(
                        f"Cannot deserialize confirmation strategy of type '{strat_class}' for tool '{tool_name}'. "
                        f"The confirmation strategy class {strat_class} does not implement a `from_dict` method."
                    )
            data["init_parameters"]["confirmation_strategies"] = deserialized_strats
        return default_from_dict(cls, data)
