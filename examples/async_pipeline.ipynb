{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/deepset-ai/haystack-experimental/blob/main/examples/async_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Running Haystack Pipelines in Asynchronous Environments\n",
    "\n",
    "In this notebook, you'll learn how to use the `AsyncPipeline` and async-enabled components from the [haystack-experimental](https://github.com/deepset-ai/haystack-experimental) repository to build and execute a Haystack pipeline in an asynchronous environment. It's based on [this short Haystack tutorial](https://haystack.deepset.ai/tutorials/27_first_rag_pipeline), so it would be a good idea to familiarize yourself with it before we begin. A further prerequisite is working knowledge of cooperative scheduling and [async programming in Python](https://docs.python.org/3/library/asyncio.html).\n",
    "\n",
    "## Motivation\n",
    "\n",
    "By default, the `Pipeline` class in `haystack` is regularly Python object class that exposes non-`async` methods to add/connect components and execute the pipeline logic. Currently, it *can* be used in async environments, but it's not optimal to do so since it executes it logic in a '[blocking](https://en.wikipedia.org/wiki/Blocking_(computing))' fashion, i.e., once the `Pipeline.run` method is invoked, it must run to completion and return the outputs before the next statement of code can be execute<sup>1</sup>. In a typical async environment, this prevents active async event loop from scheduling other `async` coroutines, thereby reducing throughput. Similarly, Haystack components currently only provide a non-`async` `run` method for their execution. To mitigate this bottleneck, we introduce the concept of async-enabled Haystack components and an `AsyncPipeline` class that cooperatively schedules the execution of both async and non-async components.\n",
    "\n",
    "### Goals\n",
    "- Allow individual components to opt into `async` support.\n",
    "    - Not all components benefit from being async-enabled - I/O-bound components are the most suitable candidates.\n",
    "- Provide a backward-compatible way to execute Haystack pipelines containing both async and non-async components.\n",
    "\n",
    "### Non-goals\n",
    "- Add async support to all existing components.\n",
    "- Execute components concurrently.\n",
    "    - While async support opens the door for concurrent execution, we're currently only focusing on providing basic `async` utility.\n",
    "\n",
    "<sup>1</sup> - This is a simplification as the Python runtime can potentially schedule another thread, but it's a detail that we can ignore in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go ahead and see what it takes to add async support to the original tutorial, starting with installing Haystack, the experimental package and the requisite dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U haystack-ai\n",
    "pip install -U haystack-experimental\n",
    "pip install datasets\n",
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide an [OpenAI API key](https://platform.openai.com/api-keys) to ensure that LLM-based evaluators can query the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
    "\n",
    "# If you're running this notebook on Google Colab, you might need to the following instead:\n",
    "#\n",
    "# from google.colab import userdata\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#  os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a `DocumentStore` to index your documents. We use the `InMemoryDocumentStore` from the `haystack-experimental` package since it has support for `async`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_experimental.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the data and convert it into Haystack `Document`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store your data in the `DocumentStore` with embeddings, initialize a `SentenceTransformersDocumentEmbedder` with the model name and call `warm_up()` to download the embedding model.\n",
    "\n",
    "Then, we calculate the embeddings of the docs with the newly warmed-up embedder and write the documents to the document store. Notice that we call the `write_documents_async` method and use the `await` keyword with it. The `DocumentStore` protocol in `haystack-experimental` exposes `async` variants of common methods such as `count_documents`, `write_documents`, etc. These [coroutines](https://docs.python.org/3/library/asyncio-task.html#coroutines) are awaitable when invoked inside an async event loop (the notebook/Google Colab kernel automatically starts an event loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "doc_embedder.warm_up()\n",
    "\n",
    "docs_with_embeddings = doc_embedder.run(docs)\n",
    "n_docs_written = await document_store.write_documents_async(docs_with_embeddings[\"documents\"])\n",
    "print(f\"Indexed {n_docs_written} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the RAG pipeline to generate answers for a user query.\n",
    "\n",
    "Initialize a text embedder to create an embedding for the user query and an `InMemoryEmbeddingRetriever` to use with the `InMemoryDocumentStore` you initialized earlier. As with the latter, the async-enabled embedding retriever class stems from the `haystack-experimental` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack_experimental.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom prompt to use with the `ChatPromptBuilder` and initialize a `OpenAIChatGenerator` to consume the output of the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_experimental.components.builders import ChatPromptBuilder\n",
    "from haystack_experimental.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack_experimental.dataclasses import ChatMessage\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = ChatPromptBuilder(template=[ChatMessage.from_user(template)])\n",
    "generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally get to the creation of the pipeline instance. Instead of using the `Pipeline` class, we use the `AsyncPipeline` class from the `haystack-experimental` package. \n",
    "\n",
    "The rest of the process, i.e., adding components and connecting them with each other remains the same as with the original `Pipeline` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_experimental.core import AsyncPipeline\n",
    "\n",
    "async_rag_pipeline = AsyncPipeline()\n",
    "# Add components to your pipeline\n",
    "async_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "async_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "async_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "async_rag_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "# Now, connect the components to each other\n",
    "async_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "async_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "async_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a coroutine that queries the pipeline with a question.\n",
    "\n",
    "The key differences between the `AsyncPipeline.run` and `Pipeline.run` methods have to do with their parameters and return values.\n",
    "\n",
    "Both `Pipeline.run` and `AsyncPipeline.run` share the `data` parameter that encapsulates the initial inputs for the pipeline's components.\n",
    "\n",
    "While `Pipeline.run` accepts an additional `include_outputs_from` parameter to return the outputs of intermediate, non-leaf components in the pipeline graph, `AsyncPipeline.run` does not. This is because the latter is implemented as an `async` generator that yields the output of **each component** as soon as it executes successfully. This has the following implications:\n",
    "\n",
    "- The output of `AsyncPipeline.run` must be consumed in an `async for` loop for the pipeline execution to make progress.\n",
    "- By providing the intermediate results as they are computed, it allows for a tighter feedback loop between the backend and the user. For example, the results of the retriever can be displayed to the user before the LLM's response is generated.\n",
    "\n",
    "Whenever a component needs to be executed, the logic of `AsyncPipeline.run` will determine if it supports async execution. \n",
    "- If the component has opted into async support, the pipeline will schedule its execution as a coroutine on the event loop and yield control back to the async scheduler until the component's outputs are returned. \n",
    "- If the component has not opted into async support, the pipeline will launch its execution in a separate thread and schedule it on the event loop.\n",
    "\n",
    "In both cases, given an `AsyncPipeline` only one of its components can be executing at any given time. However, this does not prevent multiple, different `AsyncPipeline` instances from executing concurrently.\n",
    "\n",
    "The execution of an `AsyncPipeline` is deemed to be complete once program flow exits the `async for` loop. At this point, the final results of the pipeline (the outputs of the leaf nodes in the pipeline graph) can be accessed with the loop variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "async def query_pipeline(question: str) -> Dict[str, Dict[str, Any]]:\n",
    "    input = {\n",
    "        \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    "\n",
    "    result_idx = 0\n",
    "    # The AsyncPipeline.run() method is an async generator that yields the output of each component.\n",
    "    async for pipeline_output in async_rag_pipeline.run(input):\n",
    "        print(f\"Pipeline result '{result_idx}' = {pipeline_output}\")\n",
    "        result_idx += 1\n",
    "\n",
    "    # The last output of the pipeline is the final pipeline output.\n",
    "    return pipeline_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now execute the pipeline with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Where is Gardens of Babylon?\",\n",
    "    \"Why did people build Great Pyramid of Giza?\",\n",
    "    \"What does Rhodes Statue look like?\",\n",
    "    \"Why did people visit the Temple of Artemis?\",\n",
    "    \"What is the importance of Colossus of Rhodes?\",\n",
    "    \"What happened to the Tomb of Mausolus?\",\n",
    "    \"How did Colossus of Rhodes collapse?\",\n",
    "]\n",
    "\n",
    "async def run_query_pipeline():\n",
    "    global examples\n",
    "    for question in examples:\n",
    "        print(f\"Querying pipeline with question: '{question}'\")\n",
    "        response = await query_pipeline(question)\n",
    "        print(f'\\tOutput: {response[\"llm\"][\"replies\"][0]}\\n')\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "await run_query_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively use the `run_async_pipeline` helper function to execute an `AsyncPipeline` in the same manner as a regular `Pipeline` while retaining the benefits of cooperative scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_experimental.core import run_async_pipeline\n",
    "\n",
    "question = examples[0]\n",
    "outputs = await run_async_pipeline(\n",
    "    async_rag_pipeline,\n",
    "    {\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}},\n",
    "    include_outputs_from={\"retriever\"},\n",
    ")\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Asynchronous Components\n",
    "\n",
    "Individual components can opt into async by implementing a `run_async` coroutine that has the same signature, i.e., input parameters and outputs as the `run` method. This constraint is placed to ensure that pipeline connections the same irrespective of whether a component supports async execution, allowing for plug-n-play backward compatibility with existing pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from haystack import component\n",
    "\n",
    "@component\n",
    "class MyCustomComponent:\n",
    "    def __init__(self, my_custom_param: str):\n",
    "        self.my_custom_param = my_custom_param\n",
    "\n",
    "    @component.output_types(original=str, concatenated=str)\n",
    "    def run(self, input: str) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"original\": input,\n",
    "            \"concatenated\": input + self.my_custom_param\n",
    "        }\n",
    "\n",
    "    async def do_io_bound_op(self, input: str) -> str:\n",
    "        # Do some IO-bound operation here\n",
    "        return input + self.my_custom_param\n",
    "\n",
    "    @component.output_types(original=str, concatenated=str)\n",
    "    async def run_async(self, input: str) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"original\": input,\n",
    "            \"concatenated\": await self.do_io_bound_op(input)\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
