{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/deepset-ai/haystack-experimental/blob/main/examples/async_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Running Haystack Pipelines in Asynchronous Environments\n",
    "\n",
    "In this notebook, you'll learn how to use the `AsyncPipeline` and async-enabled components from the [haystack-experimental](https://github.com/deepset-ai/haystack-experimental) repository to build and execute a Haystack pipeline in an asynchronous environment. It's based on [this short Haystack tutorial](https://haystack.deepset.ai/tutorials/27_first_rag_pipeline), so it would be a good idea to familiarize yourself with it before we begin. A further prerequisite is working knowledge of cooperative scheduling and [async programming in Python](https://docs.python.org/3/library/asyncio.html).\n",
    "\n",
    "## Motivation\n",
    "\n",
    "By default, the `Pipeline` class in `haystack` is a regular Python object class that exposes non-`async` methods to add/connect components and execute the pipeline logic. Currently, it *can* be used in async environments, but it's not optimal to do so since it executes its logic in a '[blocking](https://en.wikipedia.org/wiki/Blocking_(computing))' fashion, i.e., once the `Pipeline.run` method is invoked, it must run to completion and return the outputs before the next statement of code can be executed<sup>1</sup>. In a typical async environment, this prevents active async event loop from scheduling other `async` coroutines, thereby reducing throughput. Similarly, Haystack components currently only provide a non-`async` `run` method for their execution. To mitigate this bottleneck, we introduce the concept of async-enabled Haystack components and an `AsyncPipeline` class that cooperatively schedules the execution of both async and non-async components.\n",
    "\n",
    "### Goals\n",
    "- Allow individual components to opt into `async` support.\n",
    "    - Not all components benefit from being async-enabled - I/O-bound components are the most suitable candidates.\n",
    "- Provide a backward-compatible way to execute Haystack pipelines containing both async and non-async components.\n",
    "- Execute components concurrently\n",
    "    - Concurrent execution can yield major speed-ups in pipeline runtime (e.g. hybrid retrieval, running Generators concurrently)\n",
    "\n",
    "### Non-goals\n",
    "- Add async support to all existing components.\n",
    "\n",
    "<sup>1</sup> - This is a simplification as the Python runtime can potentially schedule another thread, but it's a detail that we can ignore in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go ahead and see what it takes to add async support to the original tutorial, starting with installing Haystack, the experimental package and the requisite dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -U haystack-ai\n",
    "pip install -U haystack-experimental\n",
    "pip install datasets\n",
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide an [OpenAI API key](https://platform.openai.com/api-keys) to ensure that LLM generator can query the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key:\")\n",
    "\n",
    "# If you're running this notebook on Google Colab, you might need to the following instead:\n",
    "#\n",
    "# from google.colab import userdata\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#  os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a `DocumentStore` to index your documents. We use the `InMemoryDocumentStore` from the `haystack-experimental` package since it has support for `async`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_experimental.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the data and convert it into Haystack `Document`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "docs = [Document(content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store your data in the `DocumentStore` with embeddings, initialize a `SentenceTransformersDocumentEmbedder` with the model name and call `warm_up()` to download the embedding model.\n",
    "\n",
    "Then, we calculate the embeddings of the docs with the newly warmed-up embedder and write the documents to the document store. Notice that we call the `write_documents_async` method and use the `await` keyword with it. The `DocumentStore` protocol in `haystack-experimental` exposes `async` variants of common methods such as `count_documents`, `write_documents`, etc. These [coroutines](https://docs.python.org/3/library/asyncio-task.html#coroutines) are awaitable when invoked inside an async event loop (the notebook/Google Colab kernel automatically starts an event loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d2b8f867b249b9b394312fa8d9a154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 151 documents\n"
     ]
    }
   ],
   "source": [
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "doc_embedder.warm_up()\n",
    "\n",
    "docs_with_embeddings = doc_embedder.run(docs)\n",
    "n_docs_written = await document_store.write_documents_async(docs_with_embeddings[\"documents\"])\n",
    "print(f\"Indexed {n_docs_written} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the RAG pipeline to generate answers for a user query.\n",
    "\n",
    "Initialize a text embedder to create an embedding for the user query and an `InMemoryEmbeddingRetriever` to use with the `InMemoryDocumentStore` you initialized earlier. As with the latter, the async-enabled embedding retriever class stems from the `haystack-experimental` package."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "\n",
    "from haystack_experimental.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a custom prompt to use with the `ChatPromptBuilder` and initialize a `OpenAIChatGenerator` to consume the output of the former."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "from haystack_experimental.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = ChatPromptBuilder(template=[ChatMessage.from_user(template)])\n",
    "generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We finally get to the creation of the pipeline instance. Instead of using the `Pipeline` class, we use the `AsyncPipeline` class from the `haystack-experimental` package. \n",
    "\n",
    "The rest of the process, i.e., adding components and connecting them with each other remains the same as with the original `Pipeline` class."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from haystack_experimental.core import AsyncPipeline\n",
    "\n",
    "async_rag_pipeline = AsyncPipeline()\n",
    "# Add components to your pipeline\n",
    "async_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "async_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "async_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "async_rag_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "# Now, connect the components to each other\n",
    "async_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "async_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "async_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.messages\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, we create a coroutine that queries the pipeline with a question.\n",
    "\n",
    "The key differences between the `AsyncPipeline.run` and `Pipeline.run` methods have to do with their parameters and return values.\n",
    "\n",
    "Both `Pipeline.run` and `AsyncPipeline.run` share the `data` parameter that encapsulates the initial inputs for the pipeline's components.\n",
    "\n",
    "While `Pipeline.run` accepts an additional `include_outputs_from` parameter to return the outputs of intermediate, non-leaf components in the pipeline graph, `AsyncPipeline.run` does not. This is because the latter is implemented as an `async` generator that yields the output of **each component** as soon as it executes successfully. This has the following implications:\n",
    "\n",
    "- The output of `AsyncPipeline.run` must be consumed in an `async for` loop for the pipeline execution to make progress.\n",
    "- By providing the intermediate results as they are computed, it allows for a tighter feedback loop between the backend and the user. For example, the results of the retriever can be displayed to the user before the LLM's response is generated.\n",
    "\n",
    "Whenever a component needs to be executed, the logic of `AsyncPipeline.run` will determine if it supports async execution. \n",
    "- If the component has opted into async support, the pipeline will schedule its execution as a coroutine on the event loop and yield control back to the async scheduler until the component's outputs are returned. \n",
    "- If the component has not opted into async support, the pipeline will launch its execution in a separate thread and schedule it on the event loop.\n",
    "\n",
    "In both cases, given an `AsyncPipeline` only one of its components can be executing at any given time. However, this does not prevent multiple, different `AsyncPipeline` instances from executing concurrently.\n",
    "\n",
    "The execution of an `AsyncPipeline` is deemed to be complete once program flow exits the `async for` loop. At this point, the final results of the pipeline (the outputs of the leaf nodes in the pipeline graph) can be accessed with the loop variable."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "async def query_pipeline(question: str) -> Dict[str, Dict[str, Any]]:\n",
    "    input = {\n",
    "        \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "    }\n",
    "\n",
    "    result_idx = 0\n",
    "    async for pipeline_output in async_rag_pipeline.run_async_generator(input):\n",
    "        print(f\"Pipeline result '{result_idx}' = {pipeline_output}\")\n",
    "        result_idx += 1\n",
    "\n",
    "    # The last output of the pipeline is the final pipeline output.\n",
    "    return pipeline_output"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now execute the pipeline with some examples."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "examples = [\n",
    "    \"Where is Gardens of Babylon?\",\n",
    "    \"Why did people build Great Pyramid of Giza?\",\n",
    "    \"What does Rhodes Statue look like?\",\n",
    "]\n",
    "\n",
    "async def run_query_pipeline():\n",
    "    global examples\n",
    "    for question in examples:\n",
    "        print(f\"Querying pipeline with question: '{question}'\")\n",
    "        response = await query_pipeline(question)\n",
    "        print(f'\\tOutput: {response[\"llm\"][\"replies\"][0]}\\n')\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "await run_query_pipeline()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can alternatively use the `run_async` method to execute an `AsyncPipeline` in the same manner as a regular `Pipeline` while retaining the benefits of cooperative scheduling."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "question = examples[0]\n",
    "outputs = await async_rag_pipeline.run_async(\n",
    "    {\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}},\n",
    "    include_outputs_from={\"retriever\"},\n",
    ")\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Custom Asynchronous Components\n",
    "\n",
    "Individual components can opt into async by implementing a `run_async` coroutine that has the same signature, i.e., input parameters and outputs as the `run` method. This constraint is placed to ensure that pipeline connections are the same irrespective of whether a component supports async execution, allowing for plug-n-play backward compatibility with existing pipelines.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "from haystack import component\n",
    "\n",
    "\n",
    "@component\n",
    "class MyCustomComponent:\n",
    "    def __init__(self, my_custom_param: str):\n",
    "        self.my_custom_param = my_custom_param\n",
    "\n",
    "    @component.output_types(original=str, concatenated=str)\n",
    "    def run(self, input: str) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"original\": input,\n",
    "            \"concatenated\": input + self.my_custom_param\n",
    "        }\n",
    "\n",
    "    async def do_io_bound_op(self, input: str) -> str:\n",
    "        # Do some IO-bound operation here\n",
    "        return input + self.my_custom_param\n",
    "\n",
    "    @component.output_types(original=str, concatenated=str)\n",
    "    async def run_async(self, input: str) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"original\": input,\n",
    "            \"concatenated\": await self.do_io_bound_op(input)\n",
    "        }"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Running components concurrently\n",
    "\n",
    "Components are scheduled to run concurrently, when the execution graph allows it.\n",
    "For example:\n",
    "\n",
    "In most hybrid retrieval pipelines (i.e. you run BM25 and embedding retrieval to join the results later), the calls to the document store could be executed concurrently. However, the synchronous Pipeline will execute the components sequentially. The `AsyncPipeline` can schedule the components to run concurrently. For most production-grade hybrid retrieval setups, this would reduce total pipeline runtime by 100-600 ms (depending on the speed of your document store).\n",
    "\n",
    "The difference is even more pronounced, when running pipelines with LLM calls that could execute concurrently."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import asyncio\n",
    "\n",
    "from haystack import component\n",
    "from haystack_experimental import Pipeline, AsyncPipeline\n",
    "\n",
    "# We implement a small custom component that helps us illustrate the concurrent scheduling.\n",
    "@component\n",
    "class AsyncTestComponent:\n",
    "    \"\"\"\n",
    "    A test component that simulates async operations by waiting for a specified time\n",
    "    before returning a message.\n",
    "\n",
    "    ### Usage example\n",
    "    ```python\n",
    "    test_comp = AsyncTestComponent(name=\"TestComponent\", wait_time=2)\n",
    "\n",
    "    # Sync usage\n",
    "    result = test_comp.run(user_msg=\"Hello\")\n",
    "    print(result[\"message\"])  # prints after 2 seconds\n",
    "\n",
    "    # Async usage\n",
    "    result = await test_comp.run_async(user_msg=\"Hello\")\n",
    "    print(result[\"message\"])  # prints after 2 seconds\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, wait_time: int = 1):\n",
    "        \"\"\"\n",
    "        Initialize the AsyncTestComponent.\n",
    "\n",
    "        :param name: Name of the component to be used in the output message\n",
    "        :param wait_time: Time to wait before returning result (in seconds)\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.wait_time = wait_time\n",
    "\n",
    "    @component.output_types(message=str)\n",
    "    def run(self, user_msg: str) -> dict:\n",
    "        \"\"\"\n",
    "        Synchronous method that waits for the specified time and returns a message.\n",
    "\n",
    "        :param user_msg: Input message from the user (unused in output but required for example)\n",
    "        :return: Dictionary containing the output message\n",
    "        \"\"\"\n",
    "        time.sleep(self.wait_time)\n",
    "        return {\"message\": f\"Message from {self.name}\"}\n",
    "\n",
    "    @component.output_types(message=str)\n",
    "    async def run_async(self, user_msg: str) -> dict:\n",
    "        \"\"\"\n",
    "        Asynchronous method that waits for the specified time and returns a message.\n",
    "\n",
    "        :param user_msg: Input message from the user (unused in output but required for example)\n",
    "        :return: Dictionary containing the output message\n",
    "        \"\"\"\n",
    "        await asyncio.sleep(self.wait_time)\n",
    "        return {\"message\": f\"Component {self.name}: Received '{user_msg}'.\"}\n",
    "\n",
    "def get_pipeline(type_=\"async\"):\n",
    "    wait_1 = AsyncTestComponent(name=\"wait_1\", wait_time=1)\n",
    "    wait_2 = AsyncTestComponent(name=\"wait_2\", wait_time=2)\n",
    "    wait_3 = AsyncTestComponent(name=\"wait_3\", wait_time=3)\n",
    "    wait_4 = AsyncTestComponent(name=\"wait_4\", wait_time=4)\n",
    "    wait_10 = AsyncTestComponent(name=\"wait_10\", wait_time=10)\n",
    "\n",
    "    if type_ == \"async\":\n",
    "        pp = AsyncPipeline()\n",
    "    else:\n",
    "        pp = Pipeline()\n",
    "\n",
    "    pp.add_component(\"wait_1\", wait_1)\n",
    "    pp.add_component(\"wait_2\", wait_2)\n",
    "    pp.add_component(\"wait_3\", wait_3)\n",
    "    pp.add_component(\"wait_4\", wait_4)\n",
    "    pp.add_component(\"wait_10\", wait_10)\n",
    "\n",
    "\n",
    "    return pp"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's run the sync version first\n",
    "pipe = get_pipeline(type_=\"sync\")\n",
    "\n",
    "start_time = time.time()\n",
    "pipe.run({\"user_msg\": \"Hi\"})\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time for 'Pipeline': {execution_time:.4f} seconds\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The AsyncPipeline is twice as fast!\n",
    "\n",
    "pipe = get_pipeline(\"async\")\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = await pipe.run_async({\"user_msg\": \"Hi\"})\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time for 'AsyncPipeline': {execution_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
